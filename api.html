

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>audmetric &mdash; audmetric Documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  
  
  

  

  
  
    

  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/audeering.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/audeering.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="audmetric.utils" href="api-utils.html" />
    <link rel="prev" title="Installation" href="install.html" />
    
  

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          <a href="index.html">
          
            <img src="_static/images/audeering.png" class="logo" alt="audEERING"/>
          
          
            <span> audmetric</span>
          
          </a>

          
            
            
              <div class="version">
                v1.1.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">audmetric</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#accuracy">accuracy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#concordance-cc">concordance_cc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#confusion-matrix">confusion_matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#detection-error-tradeoff">detection_error_tradeoff</a></li>
<li class="toctree-l2"><a class="reference internal" href="#edit-distance">edit_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#equal-error-rate">equal_error_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="#event-error-rate">event_error_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mean-absolute-error">mean_absolute_error</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mean-squared-error">mean_squared_error</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pearson-cc">pearson_cc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fscore-per-class">fscore_per_class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#precision-per-class">precision_per_class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#recall-per-class">recall_per_class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unweighted-average-bias">unweighted_average_bias</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unweighted-average-fscore">unweighted_average_fscore</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unweighted-average-precision">unweighted_average_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unweighted-average-recall">unweighted_average_recall</a></li>
<li class="toctree-l2"><a class="reference internal" href="#weighted-confusion-error">weighted_confusion_error</a></li>
<li class="toctree-l2"><a class="reference internal" href="#word-error-rate">word_error_rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api-utils.html">audmetric.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="genindex.html">Index</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">audmetric</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>audmetric</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/audeering/audmetric/" class="fa fa-github"> GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-audmetric">
<span id="audmetric"></span><h1>audmetric<a class="headerlink" href="#module-audmetric" title="Permalink to this heading">¶</a></h1>
<div class="section" id="accuracy">
<h2>accuracy<a class="headerlink" href="#accuracy" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.accuracy">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#accuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Classification accuracy.</p>
<div class="math notranslate nohighlight">
\[\text{accuracy} = \frac{\text{number of correct predictions}}
                       {\text{number of total predictions}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – included labels in preferred ordering.
Sample is considered in computation if either prediction or
ground truth (logical OR) is contained in labels.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>accuracy of prediction <span class="math notranslate nohighlight">\(\in [0, 1]\)</span></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="concordance-cc">
<h2>concordance_cc<a class="headerlink" href="#concordance-cc" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.concordance_cc">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">concordance_cc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#concordance_cc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.concordance_cc" title="Permalink to this definition">¶</a></dt>
<dd><p>Concordance correlation coefficient.</p>
<div class="math notranslate nohighlight">
\[\rho_c = \frac{2\rho\sigma_\text{prediction}\sigma_\text{truth}}
              {\sigma_\text{prediction}^2 + \sigma_\text{truth}^2 + (
              \mu_\text{prediction}-\mu_\text{truth})^2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho\)</span> is the Pearson correlation coefficient,
<span class="math notranslate nohighlight">\(\mu\)</span> the mean
and <span class="math notranslate nohighlight">\(\sigma^2\)</span> the variance.<a class="footnote-reference brackets" href="#footcite-lin1989" id="id1">1</a></p>
<div class="docutils container" id="id2">
<dl class="footnote brackets">
<dt class="label" id="footcite-lin1989"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Lawrence I-Kuei Lin. A concordance correlation coefficient to evaluate reproducibility. <em>Biometrics</em>, 45:255–268, 1989. <a class="reference external" href="https://doi.org/10.2307/2532051">doi:10.2307/2532051</a>.</p>
</dd>
</dl>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – ground truth values</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – predicted values</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>concordance correlation coefficient <span class="math notranslate nohighlight">\(\in [-1, 1]\)</span></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">concordance_cc</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.6666666666666666</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="confusion-matrix">
<h2>confusion_matrix<a class="headerlink" href="#confusion-matrix" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.confusion_matrix">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#confusion_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Confusion matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>normalize</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – normalize confusion matrix over the rows</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>confusion matrix</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="go">[[1, 0, 0], [0, 0, 1], [1, 0, 0]]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="detection-error-tradeoff">
<h2>detection_error_tradeoff<a class="headerlink" href="#detection-error-tradeoff" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.detection_error_tradeoff">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">detection_error_tradeoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#detection_error_tradeoff"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.detection_error_tradeoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Detection error tradeoff for verification experiments.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Detection_error_tradeoff">detection error tradeoff (DET)</a>
is a graph showing
the false non-match rate (FNMR)
against the false match rate (FMR).
The FNMR indicates
how often an enrolled speaker was missed.
The FMR indicates
how often an impostor was verified as the enrolled speaker.</p>
<p>This function does not return a figure,
but the FMR and FNMR,
together with the corresponding verification thresholds
at which a similarity value
was regarded to belong to the enrolled speaker.</p>
<p><code class="docutils literal notranslate"><span class="pre">truth</span></code> may only contain entries like <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0,</span> <span class="pre">True,</span> <span class="pre">False...]</span></code>,
whereas prediction values
can also contain similarity scores, e.g. <code class="docutils literal notranslate"><span class="pre">[0.8,</span> <span class="pre">0.1,</span> <span class="pre">...]</span></code>.</p>
<p>The implementation is identical with the one provided
by the <a class="reference external" href="https://github.com/manuelaguadomtz/pyeer">pyeer</a> package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – ground truth classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]]) – predicted classes or similarity scores</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p>false match rate (FMR)</p></li>
<li><p>false non-match rate (FNMR)</p></li>
<li><p>verification thresholds</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> contains values
    different from <code class="docutils literal notranslate"><span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">True,</span> <span class="pre">False</span></code></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detection_error_tradeoff</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="go">(array([1., 0.]), array([0., 0.]), array([0.1, 0.9]))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="edit-distance">
<h2>edit_distance<a class="headerlink" href="#edit-distance" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.edit_distance">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">edit_distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#edit_distance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.edit_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Edit distance between two strings of characters or sequences of ints.</p>
<p>The implementation follows the <a class="reference external" href="https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm">Wagner-Fischer algorithm</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – ground truth sequence</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – predicted sequence</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>edit distance</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="s1">&#39;lorem&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="s1">&#39;lorm&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">edit_distance</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">edit_distance</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="equal-error-rate">
<h2>equal_error_rate<a class="headerlink" href="#equal-error-rate" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.equal_error_rate">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">equal_error_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#equal_error_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.equal_error_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Equal error rate for verification tasks.</p>
<p>The equal error rate (EER) is the point
where false non-match rate (FNMR)
and the impostors or false match rate (FMR)
are identical.
The FNMR indicates
how often an enrolled speaker was missed.
The FMR indicates
how often an impostor was verified as the enrolled speaker.</p>
<p>In practice the score distribution is not continuous
and an interval is returned instead.
The EER value will be set as the midpoint
of this interval:<a class="footnote-reference brackets" href="#footcite-maio2002" id="id3">2</a></p>
<div class="math notranslate nohighlight">
\[\text{EER} = \frac{
    \min(\text{FNMR}[t], \text{FMR}[t])
    + \max(\text{FNMR}[t], \text{FMR}[t])
}{2}\]</div>
<p>with <span class="math notranslate nohighlight">\(t = \text{argmin}(|\text{FNMR} - \text{FMR}|)\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">truth</span></code> may only contain entries like <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0,</span> <span class="pre">True,</span> <span class="pre">False...]</span></code>,
whereas prediction values
can also contain similarity scores, e.g. <code class="docutils literal notranslate"><span class="pre">[0.8,</span> <span class="pre">0.1,</span> <span class="pre">...]</span></code>.</p>
<p>The implementation is identical with the one provided
by the <a class="reference external" href="https://github.com/manuelaguadomtz/pyeer">pyeer</a> package.</p>
<div class="docutils container" id="id4">
<dl class="footnote brackets">
<dt class="label" id="footcite-maio2002"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>D. Maio, D. Maltoni, R. Cappelli, J. L. Wayman, and A. K. Jain. Fvc2000: fingerprint verification competition. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 24:402–412, 2002. <a class="reference external" href="https://doi.org/10.1109/34.990140">doi:10.1109/34.990140</a>.</p>
</dd>
</dl>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – ground truth classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]]) – predicted classes or similarity scores</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">namedtuple</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p>equal error rate (EER)</p></li>
<li><p>namedtuple containing
<code class="docutils literal notranslate"><span class="pre">fmr</span></code>,
<code class="docutils literal notranslate"><span class="pre">fnmr</span></code>,
<code class="docutils literal notranslate"><span class="pre">thresholds</span></code>,
<code class="docutils literal notranslate"><span class="pre">threshold</span></code>
whereas the last one corresponds to the threshold
corresponding to the returned EER</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> contains values
    different from <code class="docutils literal notranslate"><span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">True,</span> <span class="pre">False</span></code></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eer</span><span class="p">,</span> <span class="n">stats</span> <span class="o">=</span> <span class="n">equal_error_rate</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eer</span>
<span class="go">0.16666666666666666</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">threshold</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="event-error-rate">
<h2>event_error_rate<a class="headerlink" href="#event-error-rate" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.event_error_rate">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">event_error_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#event_error_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.event_error_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Event error rate based on edit distance.</p>
<p>The event error rate is computed by aggregating the mean edit
distances of each (truth, prediction)-pair and averaging the
aggregated score by the number of pairs.</p>
<p>The mean edit distance of each (truth, prediction)-pair is computed
as an average of the edit distance over the length of the longer sequence
of the corresponding pair. By normalizing over the longer sequence the
normalized distance is bound to [0, 1].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]]) – ground truth classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]]) – predicted classes</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>event error rate</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">event_error_rate</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]])</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">event_error_rate</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="go">0.25</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">event_error_rate</span><span class="p">([</span><span class="s1">&#39;lorem&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;lorm&#39;</span><span class="p">])</span>
<span class="go">0.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">event_error_rate</span><span class="p">([</span><span class="s1">&#39;lorem&#39;</span><span class="p">,</span> <span class="s1">&#39;ipsum&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;lorm&#39;</span><span class="p">,</span> <span class="s1">&#39;ipsum&#39;</span><span class="p">])</span>
<span class="go">0.1</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mean-absolute-error">
<h2>mean_absolute_error<a class="headerlink" href="#mean-absolute-error" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.mean_absolute_error">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">mean_absolute_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#mean_absolute_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.mean_absolute_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean absolute error.</p>
<div class="math notranslate nohighlight">
\[\text{MAE} = \frac{1}{n} \sum^n_{i=1}
    |\text{prediction} - \text{truth}|\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – ground truth values</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – predicted values</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mean absolute error</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mean-squared-error">
<h2>mean_squared_error<a class="headerlink" href="#mean-squared-error" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.mean_squared_error">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">mean_squared_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#mean_squared_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.mean_squared_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean squared error.</p>
<div class="math notranslate nohighlight">
\[\text{MSE} = \frac{1}{n} \sum^n_{i=1}
    (\text{prediction} - \text{truth})^2\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – ground truth values</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – predicted values</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mean squared error</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pearson-cc">
<h2>pearson_cc<a class="headerlink" href="#pearson-cc" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.pearson_cc">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">pearson_cc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#pearson_cc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.pearson_cc" title="Permalink to this definition">¶</a></dt>
<dd><p>Pearson correlation coefficient.</p>
<div class="math notranslate nohighlight">
\[\rho = \frac{\text{cov}(\text{prediction}, \text{truth})}{
\sigma_\text{prediction}\sigma_\text{truth}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation,
and <span class="math notranslate nohighlight">\(\text{cov}\)</span> is the covariance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – ground truth values</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – predicted values</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>pearson correlation coefficient <span class="math notranslate nohighlight">\(\in [-1, 1]\)</span></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pearson_cc</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.8660254037844385</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fscore-per-class">
<h2>fscore_per_class<a class="headerlink" href="#fscore-per-class" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.fscore_per_class">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">fscore_per_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#fscore_per_class"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.fscore_per_class" title="Permalink to this definition">¶</a></dt>
<dd><p>F-score per class.</p>
<div class="math notranslate nohighlight">
\[\text{fscore}_k = \frac{\text{true positive}_k}
         {\text{true positive}_k + \frac{1}{2}
         (\text{false positive}_k + \text{false negative}_k)}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>zero_division</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – set the value to return when there is a zero division</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>dictionary with label as key and F-score as value</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fscore_per_class</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">{0: 0.6666666666666666, 1: 0.0}</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="precision-per-class">
<h2>precision_per_class<a class="headerlink" href="#precision-per-class" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.precision_per_class">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">precision_per_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#precision_per_class"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.precision_per_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision per class.</p>
<div class="math notranslate nohighlight">
\[\text{precision}_k = \frac{\text{true positive}_k}
         {\text{true positive}_k + \text{false positive}_k}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>zero_division</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – set the value to return when there is a zero division</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>dictionary with label as key and precision as value</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">precision_per_class</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">{0: 1.0, 1: 0.0}</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="recall-per-class">
<h2>recall_per_class<a class="headerlink" href="#recall-per-class" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.recall_per_class">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">recall_per_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#recall_per_class"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.recall_per_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Recall per class.</p>
<div class="math notranslate nohighlight">
\[\text{recall}_k = \frac{\text{true positive}_k}
         {\text{true positive}_k + \text{false negative}_k}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>zero_division</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – set the value to return when there is a zero division</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>dictionary with label as key and recall as value</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">recall_per_class</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">{0: 0.5, 1: 0.0}</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unweighted-average-bias">
<h2>unweighted_average_bias<a class="headerlink" href="#unweighted-average-bias" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.unweighted_average_bias">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">unweighted_average_bias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">protected_variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subgroups=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric=&lt;function</span> <span class="pre">fscore_per_class&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction=&lt;function</span> <span class="pre">std&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#unweighted_average_bias"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.unweighted_average_bias" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute unweighted average bias with respect to a protected variable.</p>
<p>The bias is measured in terms of <em>equalized odds</em> which requires
the classifier to have identical performance for all classes independent
of a protected variable such as race. The performance of the classifier
for its different classes can be assessed with standard metrics
such as <em>recall</em> or <em>precision</em>. The difference in performance, denoted
as score divergence, can be computed in different ways, as well.
For two subgroups the (absolute) difference serves as a standard choice.
For more than two subgroups the score divergence could be estimated by
the standard deviation of the scores.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If for a class less than two subgroups exhibit a performance score,
the corresponding class is ignored in the bias computation.
This occurs if there is no class sample for a subgroup,
e.g. no negative (class label) female (subgroup of sex).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted classes</p></li>
<li><p><strong>protected_variable</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – manifestations of protected variable such as
subgroups “male” and “female” of variable “sex”</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
The bias is computed only on the specified labels.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>subgroups</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included subgroups in preferred ordering.
The direction of the bias is determined by the ordering of the
subgroups.
Besides, the bias is computed only on the specified subgroups.
If no subgroups are supplied, they will be inferred from
<span class="math notranslate nohighlight">\(\text{protected\_variable}\)</span> and ordered alphanumerically.</p></li>
<li><p><strong>metric</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]) – metric which equalized odds are measured with.
Typical choices are: <a class="reference internal" href="#audmetric.recall_per_class" title="audmetric.recall_per_class"><code class="xref py py-func docutils literal notranslate"><span class="pre">audmetric.recall_per_class()</span></code></a>,
<a class="reference internal" href="#audmetric.precision_per_class" title="audmetric.precision_per_class"><code class="xref py py-func docutils literal notranslate"><span class="pre">audmetric.precision_per_class()</span></code></a> or
<a class="reference internal" href="#audmetric.fscore_per_class" title="audmetric.fscore_per_class"><code class="xref py py-func docutils literal notranslate"><span class="pre">audmetric.fscore_per_class()</span></code></a></p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – specifies the reduction operation to measure the divergence
between the scores of the subgroups of the protected variable
for each class. Typical choices are:
difference or absolute difference between scores for two subgroups
and standard deviation of scores for more than two subgroups.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>unweighted average bias</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code>, <code class="docutils literal notranslate"><span class="pre">prediction</span></code> and <code class="docutils literal notranslate"><span class="pre">protected_variable</span></code>
    have different lengths</p></li>
<li><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">subgroups</span></code> contains values not contained in
    <code class="docutils literal notranslate"><span class="pre">protected_variable</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_bias</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">])</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_bias</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">subgroups</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">reduction</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">-1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_bias</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">metric</span><span class="o">=</span><span class="n">recall_per_class</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">nan</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_bias</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">metric</span><span class="o">=</span><span class="n">recall_per_class</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unweighted-average-fscore">
<h2>unweighted_average_fscore<a class="headerlink" href="#unweighted-average-fscore" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.unweighted_average_fscore">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">unweighted_average_fscore</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#unweighted_average_fscore"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.unweighted_average_fscore" title="Permalink to this definition">¶</a></dt>
<dd><p>Unweighted average F-score.</p>
<div class="math notranslate nohighlight">
\[\text{UAF} = \frac{1}{K} \sum^K_{k=1}
    \frac{\text{true positive}_k}
         {\text{true positive}_k + \frac{1}{2}
         (\text{false positive}_k + \text{false negative}_k)}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>zero_division</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – set the value to return when there is a zero division</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>unweighted average precision</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_fscore</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.3333333333333333</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unweighted-average-precision">
<h2>unweighted_average_precision<a class="headerlink" href="#unweighted-average-precision" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.unweighted_average_precision">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">unweighted_average_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#unweighted_average_precision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.unweighted_average_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Unweighted average precision.</p>
<div class="math notranslate nohighlight">
\[\text{UAP} = \frac{1}{K} \sum^K_{k=1}
    \frac{\text{true positive}_k}
         {\text{true positive}_k + \text{false positive}_k}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>zero_division</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – set the value to return when there is a zero division</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>unweighted average precision</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_precision</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unweighted-average-recall">
<h2>unweighted_average_recall<a class="headerlink" href="#unweighted-average-recall" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.unweighted_average_recall">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">unweighted_average_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#unweighted_average_recall"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.unweighted_average_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Unweighted average recall.</p>
<div class="math notranslate nohighlight">
\[\text{UAR} = \frac{1}{K} \sum^K_{k=1}
    \frac{\text{true positive}_k}
         {\text{true positive}_k + \text{false negative}_k}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
<li><p><strong>zero_division</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – set the value to return when there is a zero division</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>unweighted average recall</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unweighted_average_recall</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.25</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="weighted-confusion-error">
<h2>weighted_confusion_error<a class="headerlink" href="#weighted-confusion-error" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.weighted_confusion_error">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">weighted_confusion_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#weighted_confusion_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.weighted_confusion_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Weighted confusion error.</p>
<p>Computes the normalized confusion matrix, applies given weights to each
cell and sums them up. Weights are expected as positive numbers and
will be normalized by the sum of all weights. The higher the weight,
the more costly will be the error. A weight of 0 means that the cell
is not taken into account for the error, this is usually the case for the
diagonal as it holds correctly classified samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – ground truth values/classes</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – predicted values/classes</p></li>
<li><p><strong>weights</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]]) – weights applied to the confusion matrix.
Expected as a list of lists in the following form
(r=row, c=column):
<code class="docutils literal notranslate"><span class="pre">[[w_r0_c0,</span> <span class="pre">...,</span> <span class="pre">w_r0_cN],</span> <span class="pre">...,</span> <span class="pre">[w_rN_c0,</span> <span class="pre">...,</span> <span class="pre">w_rN_cN]]</span></code></p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – included labels in preferred ordering.
If no labels are supplied,
they will be inferred from
<span class="math notranslate nohighlight">\(\{\text{prediction}, \text{truth}\}\)</span>
and ordered alphabetically.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>weighted confusion error</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># penalize only errors &gt; 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weighted_confusion_error</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="word-error-rate">
<h2>word_error_rate<a class="headerlink" href="#word-error-rate" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="audmetric.word_error_rate">
<span class="sig-prename descclassname"><span class="pre">audmetric.</span></span><span class="sig-name descname"><span class="pre">word_error_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/audmetric/core/api.html#word_error_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#audmetric.word_error_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Word error rate based on edit distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – ground truth strings</p></li>
<li><p><strong>prediction</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – predicted strings</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>word error rate</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if <code class="docutils literal notranslate"><span class="pre">truth</span></code> and <code class="docutils literal notranslate"><span class="pre">prediction</span></code> differ in length</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">truth</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;lorem&#39;</span><span class="p">,</span> <span class="s1">&#39;ipsum&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;north&#39;</span><span class="p">,</span> <span class="s1">&#39;wind&#39;</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;sun&#39;</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;lorm&#39;</span><span class="p">,</span> <span class="s1">&#39;ipsum&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;north&#39;</span><span class="p">,</span> <span class="s1">&#39;wind&#39;</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_error_rate</span><span class="p">(</span><span class="n">truth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="api-utils.html" class="btn btn-neutral float-right" title="audmetric.utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="install.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div>
    <p>
        
        
        
          Built with <a href="https://www.sphinx-doc.org/en/master/">Sphinx</a> on 2022/07/05 using the <a href="https://github.com/audeering/sphinx-audeering-theme/">audEERING theme</a>
        
    </p>
  </div>

  <div role="contentinfo">
    <p>
        
      &copy; 2019-2022 audEERING GmbH
    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  



  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>